{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and constants\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from scipy import signal\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "WAV_PATH = \"CREMA-D/AudioWAV/\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detele all variables\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cosmin/gender_classification/venv/lib/python3.12/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "with open(\"CREMA-D/VideoDemographics.csv\") as csv_file:\n",
    "    label_to_num = {\"Male\": 0, \"Female\": 1}\n",
    "    sexes = pandas.read_csv(csv_file).to_dict(orient='list')['Sex']\n",
    "    data, labels = [], []\n",
    "    maxl = 0\n",
    "\n",
    "    for file in os.listdir(WAV_PATH):\n",
    "        # Load the audio with the given sample rate and mono=False ()\n",
    "        audio = librosa.load(WAV_PATH + file, sr=SAMPLE_RATE, mono=False)[0]\n",
    "        mfcc =torchaudio.transforms.MFCC(sample_rate=SAMPLE_RATE)(torch.tensor(audio))\n",
    "        data.append(mfcc)\n",
    "\n",
    "        labels.append(label_to_num[sexes[int(file.split('_')[0][-1]) - 1]])\n",
    "\n",
    "        maxl = max(maxl, mfcc.size(1))\n",
    "\n",
    "    data = [pad(mfcc, (0, maxl - mfcc.size(1))) if mfcc.size(1) < maxl else mfcc for mfcc in data]\n",
    "\n",
    "    split_idx = int(2 / 3 * len(data)) + 1\n",
    "    train_features, train_labels = data[ : split_idx], labels[ : split_idx]\n",
    "    test_features, test_labels = data[split_idx : ], labels[split_idx : ]\n",
    "\n",
    "train_features = torch.stack(train_features)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_features = torch.stack(test_features)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = AudioDataset(train_features, train_labels)\n",
    "test_dataset = AudioDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GenderClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenderClassificationModel, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64000, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = GenderClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.8541514118894553\n",
      "Epoch 2, Loss: 0.5170261570467398\n",
      "Epoch 3, Loss: 0.48819110771784413\n",
      "Epoch 4, Loss: 0.466637743016084\n",
      "Epoch 5, Loss: 0.44183105325851685\n",
      "Epoch 6, Loss: 0.39274306022203886\n",
      "Epoch 7, Loss: 0.3296114689646623\n",
      "Epoch 8, Loss: 0.2605560854889261\n",
      "Epoch 9, Loss: 0.1855167842302949\n",
      "Epoch 10, Loss: 0.12526839772143808\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train().to(device=DEVICE)\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.54032258064517%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
